%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Gaussian Mixture Models}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

\section{Mixture Distributions}

As shown in the last chapter the joint probability distribution of the complete data is not strictly concave thus we can not guarantee to converge to the global optima. One thing that we can do is to initialise the parameters multiple times randomly to see if the maxima of the likelihood function increases. Fig 2.2 appropriately displays the shape of the joint probability distribution of the complete data. There are visible multiple local optima of the log-likelihood function caused by the hidden states. Such probability distributions are often called mixture distributions. 

They were implicitly introduced in the last chapter while estimating the parameters of the Hidden Markov Model. Although, most of the time we consider unimodal distributions for our data since it may be justifiable empirically or considering the mixture distributions would unnecessarily complicate the computation for an indistinguishable improvement, the statistical inferences about the subpopulations within the population require such tools in cases where the subpopulations significantly differ. However, in these models we pose no requirement of the knowledge about the number of subpopulations within the population or their actual distribution. They are interpreted the same way as for the HMM as hidden variables.





%!TEX root = ../thesis.tex
\chapter{Mixture Models}

\if
pdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

As shown in the last chapter the joint probability distribution of the complete 
data is not strictly concave thus we can not guarantee the convergence to the global optimum. 
Approach we might take is to randomly initialize the parameters multiple times to see 
if the maxima of the log likelihood function increases. 
Fig 2.2 appropriately displays possible shape of the joint probability distribution of the complete data. 
There are visible multiple local optima of the log-likelihood function caused by the hidden states. 
Such probability distributions are often called mixture distributions. 

They were implicitly introduced in the last chapter while estimating the parameters of the Hidden Markov Model. 
Although, most of the time we consider unimodal distributions for our data since it may be justifiable empirically 
or considering the mixture distributions could unnecessarily complicate the computation 
for an indistinguishable improvement, the statistical inferences about the subpopulations 
within the population require such tools in cases where the subpopulations significantly differ.
However, in these models we pose no requirement of the knowledge about the number of subpopulations 
within the population or their actual distribution. They are interpreted the same way as for the HMM 
as hidden variables.

\section{Mixture Distributions}

Let $Y_j$ denote a p-dimensional random vector with probability density function $f(y_j)$ on $\mathbb{R}^p$  
selected from random sample $Y_1,\ldots,Y_n$. A realization of such a random vector is denoted by $y_j$.

Mixture distribution or density is defined as a convex combination of $g$ component distributions or densities respectively 
as follows:

\begin{equation}
f(y_j) = \sum_{i=1}^{g} \pi_i f_i(y_j)
\end{equation}

where $f_i(y_j)$ are component densities of the mixture and $\pi_i$ are mixing proportions or mixture weights with following properties:

\begin{equation}
    0 \leq \pi_i \leq 1 \quad \forall i \in \{1,2,\ldots,g\}
\end{equation}

and

\begin{equation}
    \sum_{i=1}^{g} \pi_i = 1
\end{equation}

Therefore, the density (3.1) of $f(y_j)$ is referred to as a g-component finite mixture density, conversely $F(y_j)$ 
as a g-component finite mixture distribution. 

Formula (3.1) assumes that the number of components $g$ is fixed. In many applications this is not the case, and we have to infer 
the number of components from the data. Furthermore, mixing proportions $\pi_i$ are also unknown and have to be estimated along with 
the respective parameters of the component densities $f_i(y_j)$.

Analogously to previous definition we can define a finite mixture of random variables $Y_j$ as follows:

\begin{equation}
    f(y_j,z) =  g(z) f(y_j|z)
\end{equation}

where $Z$ is defined as a random variable with multinomial distribution with a vector of parameters $\pi = \{\pi_1,\ldots,\pi_g\}$ and $g(z)$ as its probability density function.
Summing over all possible values of $Z$ we obtain the marginal density of random vector $Y_j$:

\begin{equation}
    f(y_j) = \sum_{i=1}^{g} P(Z=i) f(y_j|Z=i)
\end{equation}
    
where $P(Z=i) = \pi_i$ and $f(y_j|Z=i) = f_i(y_j)$ are the mixing proportions and component densities respectively Therefore
seeing the equivalence of Equation (3.1) and (3.4).

\section{Gaussian Mixture Models}
The Gaussian mixture model (GMM) is a probabilistic model that assumes all the data points are generated 
from a mixture of a finite number of Gaussian distributions. The component distributions are often chosen to be members of the same parametric family,
such as Gaussian distributions with respective mean and covariance parameter. Assuming that the number of components is $K$
the probability density function of the Gaussian mixture model can be expressed as follows:

\begin{equation}
    f(x|\mu,\Sigma) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)
\end{equation}

where $\mu_k$ and $\Sigma_k$ are the mean and covariance matrix (symmetric and positive semi-definite) of the $k$-th Gaussian component. 

Thus, the vector of parameters is $\Theta = \{k \in \mathbb{N}: (\pi_k,\mu_k,\Sigma_k)\}$. Probability density function of each component of GMM depends on the dimensionality of the data.
Let the dimensionality of the data be $d>1$ then the random vector $Y=(y_1,\ldots,y_d)$ has the probability density function of the $k$-th component 
defined as following multivariate Gaussian distribution:

\begin{equation}
    f_k(y) = \frac{1}{(2\pi)^{d/2}|\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(y_j-\mu_k)^T\Sigma_k^{-1}(y_j-\mu_k)\right)
\end{equation}

where $|\Sigma_k|$ denotes the determinant of the covariance matrix $\Sigma_k$.

\subsection{Motivating EM algorithm}

Given the data $\textbf{Y} = \{\textbf{Y}_1,\ldots,\textbf{Y}_N\}$, where each element is an independently and identically distributed 
p-dimensional random vector, s.t. $Y_n \in \mathbb{R}^p$ and:

\begin{equation}
    \textbf{Y}_n \sim \mathcal{N}_p(\mu, \Sigma) \quad \forall n \in \{1,\ldots,N\} 
\end{equation}

The goal is to estimate $\Theta$ of the mixture model, 
i.e. for each mixture component $k$ we need to estimate its mean $\mu_k$, covariance matrix $\Sigma_k$ and mixing proportion $\pi_k$. 

Note that now we will assume that each element of random vector $\textbf{Y}_n$ is generated by one of the mixture components, and we know which one. 
This is called the complete data since we have information about the hidden variable $Z$ which indicates the component from which the data point was generated.
Therefore, the complete data likelihood function is defined as follows: 

\begin{equation}
    L_c(\Theta|\textbf{Y},\textbf{Z}) = \prod_{i=1}^{N} \sum_{k=1}^{K} \pi_k \mathcal{N}(\textbf{Y}_i|\mu_k,\Sigma_k)
\end{equation}

and the log-likelihood function is:

\begin{equation}
    \ell_c(\Theta|\textbf{Y},\textbf{Z}) = \sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_k \mathcal{N}(\textbf{Y}_n|\mu_k,\Sigma_k)
\end{equation}

Equation (3.10), shows that the log-likelihood function has a form of a sum of logarithms which results in no strict analytical solution for the maximum likelihood estimation of model parameters.

Given the assumption that we know the hidden variable $\textbf{Z}$ directly from the data defined as a K-dimensional random vector $\textbf{Z}=(z_1,\ldots,z_K)$ 
with multionomial distribution and paramaters $\boldsymbol{\pi} = \{\pi_1,\ldots,\pi_K\}$, the log-likelihood function can be rewritten as follows:

\begin{equation}
    \ell_c(\Theta|\textbf{Y},\textbf{Z}) = \sum_{i=1}^{N} \sum_{k=1}^{K} z_{ik} \left(\log \pi_k + \log \mathcal{N}(\textbf{Y}_n|\mu_k,\Sigma_k)\right)
\end{equation}

with variable $z_{ik}$ defined as follows:

\begin{equation}
    z_{ik} = \begin{cases}
        1 & \text{if } \textbf{Y}_i \text{ was generated by the } k\text{-th component} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

also notice that for each sample, random vector $z_{i}$ is distributed identically and independently according to the multinomial distribution with vector of parameters $\boldsymbol{\pi}$.

\subsection{EM algorithm for GMM}

The EM algorithm for GMM is very similar to the one defined for Hidden Markov Models in the previous chapter. 
We start with the definition of the $\textbf{E-step}$ of the EM algorithm.The conditional expected complete 
data log-likelihood function given log-likelihood function by Equation (3.11) as follows:

\begin{equation}
    Q(\Theta|\Theta^{(t)}) = \mathbb{E}[\ell_c(\Theta|\textbf{Y},\textbf{Z})|\textbf{Y},\Theta^{(t)}]
\end{equation}

hence inserting Equation (3.11) into (3.13) we obtain:

\begin{equation}
    Q(\Theta|\Theta^{(t)}) = \sum_{i=1}^{N} \sum_{k=1}^{K} \mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}] \left(\log \pi_k + \log \mathcal{N}(\textbf{Y}_i|\mu_k,\Sigma_k)\right)
\end{equation}

where $\Theta^{(t)}$ denotes the parameter vector at the $t$-th iteration of the algorithm and $\mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}]$ is the expected value of the hidden variable $z_{ik}$ given the data $\textbf{Y}_i$ and the parameter vector $\Theta^{(t)}$.
The expected value of the hidden variable $z_{ik}$ is defined as follows:

\begin{equation}
    \mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}] = P(z_{ik} = 1|Y_i,\Theta^{(t)}) = \frac{\pi_k^{(t)} \mathcal{N}(\textbf{Y}_i|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^{K} \pi_j^{(t)} \mathcal{N}(\textbf{Y}_i|\mu_j^{(t)},\Sigma_j^{(t)})}
\end{equation}

where $\pi_k^{(t)}$, $\mu_k^{(t)}$ and $\Sigma_k^{(t)}$ are the mixing proportion, mean and covariance matrix of the $k$-th component at the $t$-th iteration of the algorithm respectively.
Taking a step back and looking at the Equation (3.5) we observe that the conditional expected value of $z_{ik}$ given data $\textbf{Y}_i$ 
and parameter vector $\Theta^{(t)}$ is the posterior probability of the hidden variable $z_{ik}$.

The $\textbf{M-step}$ of the EM algorithm then aims to maximize the conditional expected complete data log-likelihood function $Q(\Theta|\Theta^{(t)})$ with respect to the parameter vector $\Theta$.
Such maximization is formally defined as follows:

\begin{equation}
    \Theta^{(t+1)} = \underset{\Theta}{\arg\max} Q(\Theta|\Theta^{(t)})
\end{equation}

Taking partial derivatives of $Q(\Theta|\Theta^{(t)})$ with respect to the parameters $\pi_k$, $\mu_k$ and $\Sigma_k$ and setting them to zero we obtain the following equations:

\begin{equation}
    \hat{\pi}_k^{(t+1)} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}]
\end{equation}

\begin{equation}
    \hat{\mu}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}] \textbf{Y}_i}{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}]}
\end{equation}

\begin{equation}
    \hat{\Sigma}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}] (\textbf{Y}_i - \mu_k^{(t+1)})(\textbf{Y}_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\Theta^{(t)}]}
\end{equation}

\subsection{Markov Chain Monte Carlo}

Although, EM algorithm is a very powerful tool for estimating the parameters of the mixture model,
it is not guaranteed to converge to the global optimum of the log-likelihood function. Therefore, 
we will introduce a Markov Chain Monte Carlo (MCMC) method for estimating the parameters of the mixture model by 
sampling from the posterior distribution of the parameters. As opposed to the previous section, the MCMC method
would also provide a measure of uncertainty of the estimated parameters. 

\subsection{Metroplis-Hastings Algorithm}

\subsection{Gibbs Sampling}

\section{Model Selection}
%!TEX root = ../thesis.tex


\if pdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

\chapter{Hidden Markov Models}

Until now, we have considered visible states in a sense that the sequence of states was known, we refer to these models as \textit{visible Markov Models}.
In this section we will consider a situation in which we do not observe the states directly but only as a "guess" given other visible observations that are available to us. 
These "visible" observations are labeled as emissions emitted by the respective hidden state. Thus, the observations are assumed to be generated by certain hidden stochastic process, i.e. a Markov Chain.\footnote{Models that
comprise unobserved random variables, e.g. Hidden Markov Models, are called \textbf{latent variable models}, \textbf{missing data models}, or also \textbf{models with incomplete data}}
In general, we could assume that the state and emission space is not necessarily finite, but we will follow the classical definition by \cite{Rabiner1989} and \cite{Elliott1995} of Hidden Markov Models and assume that at least state space is finite.

\section{Discrete Hidden Markov Model}

Revisiting previous section, we have defined a transition matrix $\textbf{A}$ and initial distribution $\textbf{p}$ for a homogeneous Markov Chain. Both of these parameters are used to describe the hidden stochastic process.
In order to describe the emissions, we will define an $N \times M$ \textit{emission matrix} $\textbf{B}$\footnote{Also referred to as \textbf{observation matrix} and \textbf{observations} respectively} as follows:

\begin{equation}
    \textbf{B} = \begin{pmatrix}
    b_1(y_1) & b_1(y_2) & \ldots & b_1(y_M) \\
    b_2(y_1) & b_2(y_2) & \ldots & b_2(y_M) \\
    \vdots & \vdots & \ddots & \vdots \\
    b_N(y_1) & b_N(y_2) & \ldots & b_N(y_M) \\
    \end{pmatrix}
\end{equation}

\noindent where $N$ represents number of states and $M$ number of possible emission symbols.

Probabilistically each element of the matrix represents conditional probability of emitting symbol $k$ for all $k = 1,2,\ldots,M$, given state $i$: 

\begin{equation}
    b_{i}(k) = \mathbb{P}(Y_t = k|X_t = i) 
\end{equation}

As stated at the beginning of this section, observer does not have access to the hidden states, but only to the emissions emitted by the hidden states. 
Thus, observing only another stochastic process $\{Y_t, t \in \mathbb{N}_0\}$ linked to hidden Markov Chain s.t. it governs the distribution of $Y_t$. 
In other words, entire statistical inference, even in terms of the hidden Markov process, is based on the observed sequence of emissions $\{Y_t\}$.

Considering the discrete time index $t$, Hidden Markov model is bivariate discrete-time stochastic process $\{X_t,Y_t\}$, where $\{Y_t\}$ is a sequence of conditionally 
independent and identically distributed random variables with a probability distribution determined by the hidden state $i$ at time $t$. \cite{Rabiner1989}

Note that there are three common types of Hidden Markov Models depending on structure of the underlying hidden Markov Chain according to \cite{Nelwamondo2006}. These are namely (a) left-to-right, 
(b) two-parallel left-to-right and (c) ergodic. First model bears a property that the next state index is always greater than or equal to the current state index s.t. the final state is absorbing.
Two-parallel left-to-right model allows for two parallel paths taken by the Markov Chain and lastly in ergodic model, all states are connected.
For the purpose of this work we will only consider models with ergodic property as shown in Figure \ref{fig:ergodic}.

Above we specified an emission matrix $\textbf{B}$ as a discrete probability distribution of the emissions, given the hidden state, that take on values from a finite set of $M$ possible emissions. $\{Y_t\}$ is then a sequence of conditionally 
independent and identically distributed discrete random variables that follow a categorical distribution with $M$ possible outcomes, i.e. each emission symbol $k$ is given a probability of being emitted by the hidden state $i$
 as stated by \cite{Paisley2009}:

\begin{gather}
    Y | X = i \sim \text{Cat}(M;b_i(y_1),\ldots,b_i(y_M)), \quad \forall i \in I \\
    f(y|x=i) = \prod\limits_{k=1}^M b_i(k)^{\mathbbm{1}{\{y=k\}}}
\end{gather}
 
Important property of the categorical distribution in Bayesian statistics is that it is a generalization of the Bernoulli distribution for $M > 2$ 
outcomes and also a conjugate prior for the Dirichlet distribution. In some instances, as expressed by \cite{Paisley2009}, we also use Dirichlet distribution with parameters $\alpha_1=\ldots=\alpha_M$ 
as it is a special case called uniform prior for the categorical distribution. The same approach is also applicable other parameters of the model, i.e. 
transition matrix $\textbf{A}$ and initial distribution $\textbf{p}$.

Usually, as mentioned in \cite{Agresti2007}, we generalize the categorical distribution to a multinomial distribution in which case the number of samples is fixed to 1. 
In other words, the emission space is not 1-to-M encoded but rather 1-of-M encoded s.t. the probability mass function is:

\begin{gather}
    Y | X = i \sim \text{Multinomial}(1;M;b_i(y_1),\ldots,b_i(y_M)), \quad \forall i \in I \\
    f(\textbf{y},n=1|x=i) = \prod_{k=1}^M b_i(k)^{y_{k}}, \quad \sum_{k=1}^M y_{k} = 1
\end{gather}

\noindent where $\textbf{y}$ is a vector of length $M$ with only one element equal to 1 and the rest 0. 

HMM with finite state and emission space as described above is also called \textit{discrete Hidden Markov Model}. Same definition is also applicable for continuous emission space, where the probability density function is 
often a Gaussian distribution with mean vector $\mathbf{\mu}$ and covariance matrix $\Sigma$ or Poisson distribution with parameter vector $\boldsymbol{\lambda}$.

An important note, as per \cite{Bishop2006} and \cite{Ramaprasad2004}, is that the emissions are not necessarily independent, but only conditionally independent given the hidden state, which implies that the process $\{Y_t\}$ is not a Markov Chain, as opposed to process $\{X_t\}$,
but we may view it as an extension of Mixture Models with certain dependencies between the emissions. For example, in a case where we have a Markov Chain with finite $N$ hidden states and emissions modelled by Gaussian distributions, 
we refer to such model as \textit{Gaussian Hidden Markov Model}\footnote{In some literature as \cite{Capp2005} also \textbf{normal Hidden Markov Model}} s.t. marginal distribution of $Y_t$ is a mixture of Gaussian distributions.

Since HMM is categorized as a sequence model, we are generally interested in a joint probability distribution of the hidden states and the emissions when examining 
time series data or any other sequential data. Such joint probability distribution given parameters vector $\theta = (\textbf{A},\textbf{B},\textbf{p})$ is, as described by \cite{Rabiner1989}, 
expressed by the following equation:

\begin{align}
    \mathbb{P}(\textbf{X}, \textbf{Y}| \theta) = & \mathbb{P}(X_0 = x_0) \mathbb{P}(Y_0 = y_0|X_0 = x_0) \notag \\ 
                                                 & \prod_{t=1}^T \mathbb{P}(X_t = x_t|X_{t-1} = x_{t-1}) \mathbb{P}(Y_t = y_t|X_t = x_t) \\
                                               = & p_{x_0} b_{x_0}(y_0) \prod_{t=1}^T a_{x_t,x_{t-1}} b_{x_t}(y_t)
\end{align}

\noindent where $\textbf{X} = \{x_0,x_1,\ldots,x_T\}$ and $\textbf{Y} = \{y_0,y_1,\ldots,y_T\}$ are the sequences of hidden states and emissions respectively. 

One possibility of graphically representing such joint probability distribution is by using a graphical 
model as shown in Figure \ref{fig:HMM}. 

\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,semithick]
    \node[state, minimum size=1.2cm]         (A)                    {$x_{t-1}$};
    \node[state, minimum size=1.2cm]         (B) [right of=A]       {$x_{t}$};
    \node[state, minimum size=1.2cm]         (C) [right of=B]       {$x_{t+1}$};

    \node[left of=A]  (prev) {$\dots$};
    \node[right of=C] (next) {$\dots$};

    \path (prev) edge node [above] {} (A)
    (A)    edge node [above] {} (B)
    (B)    edge node [above] {} (C)
    (C)    edge node [above] {} (next);

    \node [draw,circle,below of=A, node distance=2cm, minimum size=1.2cm] (WA) {$y_{t-1}$};
    \node [draw,circle,below of=B, node distance=2cm, minimum size=1.2cm] (WB) {$y_{t}$};
    \node [draw,circle,below of=C, node distance=2cm, minimum size=1.2cm] (WC) {$y_{t+1}$};

    \path (A) edge [->] (WA)
    (B) edge [->] (WB)
    (C) edge [->] (WC);
    \end{tikzpicture}
\end{center}
\caption{An HMM with 4 hidden states and 2 discrete emissions denoted by $x_1$ and $x_2$.}
\label{fig:HMM}
\end{figure}

We may also visualize conditional relationship with a graphical model in Figure \ref{fig:ergodic} representing a structure of HMM with 3 hidden states $\{S_1,S_2,S_3\}$ and 2 emission symbols $\{E_1,E_2\}$ where the nodes represent the relationship 
imposed by the transition matrix $\textbf{A}$ and emission matrix $\textbf{B}$:

\begin{figure}[htbp]
    \begin{center}
    \begin{tikzpicture}[]
    % states
    \node[state, minimum size=1.2cm] (s1) at (0,2) {$S_1$}
        edge [loop above]  node[auto,above] {$a_{11}$}();
    \node[state, minimum size=1.2cm] (s2) at (3,2) {$S_2$}
        edge [<-,bend right=45] node[auto,swap] {$a_{12}$} (s1)
        edge [->,bend left=45] node[auto,above] {$a_{21}$} (s1)
        edge [loop above] node[auto,above] {$a_{22}$} ();
    \node[state, minimum size=1.2cm] (s3) at (6,2) {$S_3$}
        edge [<-,bend right=70] node[auto,swap] {$a_{13}$} (s1)
        edge [->,bend left=60] node[auto,above] {$a_{31}$}(s1)
        edge [<-,bend right=45] node[auto,swap] {$a_{23}$} (s2)
        edge [->,bend left=45] node[auto,above] {$a_{32}$} (s2)
        edge [loop above] node[auto,above] {$a_{33}$} ();
    % observations
    \node[observation, minimum size=1cm] (y1) at (1.5,-1.5) {$E_1$}
        edge [lightedge] node[auto,below,xshift=-20pt] {$b_{S_1}(E_1)$} (s1)
        edge [lightedge] (s2)
        edge [lightedge] (s3);
    \node[observation, minimum size=1cm] (y2) at (4.5,-1.5) {$E_2$}
        edge [lightedge] (s1)
        edge [lightedge] (s2)
        edge [lightedge] node[auto,swap] {$b_{S_3}(E_2)$} (s3);
    \end{tikzpicture}
    \end{center}
    \caption{An HMM with 3 hidden states and 2 emission symbols denoted by $E_1$ and $E_2$.}
    \label{fig:ergodic}
    \end{figure}
    

There are 3 main assumptions of Hidden Markov Models as a consequence of the properties of Markov processes,
as suggested by \cite{Oliver2013} and \cite{Capp2005}:

\begin{itemize}
    \item[1)] \textbf{Markov memoryless assumption} - this assumption states that the next hidden state $X_{t+1}$ depends only on the current state $X_t$, so that the transition probabilities are defined as:

        \begin{equation}
            \mathbb{P}(X_{t+1} = j| X_{t} = i,X_{t-1} = i_{t-1}, \ldots,X_{0} = i_0) = \mathbb{P}(X_{t+1} = j| X_{t} = i) \quad \forall i,j \in I
        \end{equation}

        It is also possible to assume that the states in HMM are dependent beyond the current state therefore giving rise to \textit{k-order HMM} 
        where the conditional distribution of the next state depends on the current state and the previous $k$ states $X_{t-k}, X_{t-k+1}, \ldots, X_{t-1}$. \cite{Capp2005}

    \item[2)] \textbf{Stationary assumption} - The transition matrix $\textbf{A}$ is time invariant s.t. transition probabilities only depend on the time interval between the transitions, 
    thus for all $t \neq s$:

        \begin{equation}
            \mathbb{P}(X_{t+1} = j| X_{t}= i) = \mathbb{P}(X_{s+1} = j| X_s = i) \quad \forall i,j \in I
        \end{equation}

        However, when we abstract from this assumption, we may consider a \textit{non-stationary HMM} where the transition matrix is time-dependent, i.e. depends on time index $t$.

    \item[3)] \textbf{Emission independence assumption} - Emissions are independent, s.t. if we have an emission sequence $\textbf{Y} = \{y_1,y_2,\ldots,y_T\}$ then:

        \begin{equation}
            \mathbb{P}(\textbf{Y}|X_1 = x_1,\ldots,X_T = x_T) = \prod_{t=1}^T \mathbb{P}(Y_t = y_t|X_t = x_t)
        \end{equation}

        When there is a dependence between the emissions in a way that the emission at time $t$ depends on the previous emissions, conditionally on the hidden state sequence $\{X_k\}$, $\{Y_k\}$
        forms a (non-homogeneous) Markov Chain, therefore jointly representing a generalization of HMM called \textit{Markov-switching Models}\footnote{Also, \textbf{Markov jump system}.}. \cite{Capp2005}

\end{itemize}

There are mainly 3 fundamental problems in HMM that need to be resolved as depicted by \cite{Oliver2013} and \cite{Ching2005}:
\begin{itemize}
\item[1.] \textbf{Evaluation problem} - Given a model denoted as $\theta = (\textbf{A},\textbf{B},\textbf{p})$ and an emission sequence 
                                        $\textbf{Y} = y_1, y_2,\ldots,y_T$, how to efficiently compute the probability that the model generated the observation 
                                        sequence, in other words, what is $\mathbb{P}(\textbf{Y}|\theta)$? 
\item[2.] \textbf{Decoding problem} - What is the most likely sequence of hidden states that could have generated the emission sequence \textbf{Y}? 
                                      Thus, we would like to find $\textbf{X} = \underset{\textbf{X}}{\arg\max} \mathbb{P}(\textbf{Y},\textbf{X}|\theta)$, 
                                      where $\textbf{X}$ is the hidden state sequence. 
\item[3.] \textbf{Learning problem} - Given a set of emission sequences find $\theta$ that best explains the emission sequence $\textbf{Y}$. 
                                      Thus, find the vector of parameters $\theta$ that maximizes $\mathbb{P}(\textbf{Y}|\theta)$. 
\end{itemize}

The most traditional approaches in solving these 3 fundamental problems differ and one may not suffice in solving all three. 
The evaluation problem is usually solved by \textbf{Forward-Backward algorithm}\footnote{Focusing on the most widely used implementation called \textbf{alpha-beta algorithm}.}, the decoding problem by well-known \textbf{Viterbi algorithm} 
and the last learning problem by \textbf{Baum-Welsch algorithm} which is a special case of Expectation-maximization (EM) algorithm.
 All three algorithms are described in the following chapter.
 
\section{Gaussian HMM}
 
If we were to consider a continuous emission space, we could plot the joint probability distribution of the hidden states and the emissions to see that 
the marginal distribution of the emissions is not that easily tractable and may not necessarily be unimodal. In \textit{Gaussian Hidden Markov Model} (GHMM) setting, 
marginal distribution of the emissions is a mixture of Gaussian distributions. \cite{Bishop2006}

Although, most of the time we consider unimodal distributions for our data it might not be the case for incomplete data, i.e. data with missing values.
For example, statistical inferences about the subpopulations within the overall population require such tools in cases where the subpopulations significantly differ 
and may be interpreted the same way as hidden variables in case of Hidden Markov Models.

In this section, we first introduce the concept of \textit{Mixture distributions}, then we will focus on \textit{Gaussian mixture models}
and their direct relation to \textit{GHMM}.

\subsection{Mixture Distributions}

Let $\textbf{Y}$ denote a p-dimensional random vector with probability density function $f_{\textbf{Y}}(y_1,\ldots,y_p)$ on $\mathbb{R}^p$. 
This probability density function is defined as a convex combination of $M$ component probability densities as follows from \citep{McLachlan2000}: 

\begin{equation} \label{eq:mixture}
    f_{\textbf{Y}}(y_1,\ldots,y_p) = \sum_{i=1}^{M} \pi_i f_i(y_1,\ldots,y_p)
\end{equation}

where $f_i$ is a component density of the mixture and $\pi_i$ a mixing proportion or mixture weight with following properties:

\begin{equation}
    0 \leq \pi_i \leq 1 \quad \forall i \in \{1,2,\ldots,M\}
\end{equation}

and

\begin{equation}
    \sum_{i=1}^{M} \pi_i = 1
\end{equation}

Therefore, the probability density $f_{\textbf{Y}}$ given by Equation \ref{eq:mixture} is referred to as a g-component finite mixture density, conversely $F_{\textbf{Y}}$ 
as a g-component finite mixture distribution. 

Equation \ref{eq:mixture} also assumes that the number of components $M$ is fixed but in many applications this is not the case, and we have to infer 
the number of components which are required to adequately describe the data. One way we can avoid this problem is by assuming that the number of components is 
sufficiently large or infinite, and then use a model selection criterion to select the number of components that best fits the data as suggested by \cite{Sammut2011} and \cite{Rasmussen1999}.
Furthermore, mixing proportions $\pi_i$ are also unknown and have to be estimated along with the respective parameters of the component densities.

Analogously to previous definition we can define a finite mixture of random vector $\textbf{Y}$ as \citep{Bishop2006}:

\begin{equation}
    f(y,x) = g(x) f(y|x)
\end{equation}

where $X$ is defined as a random variable following a multinomial distribution with a vector of parameters $\pi = \{\pi_1,\ldots,\pi_M\}$ and $g(x)$ as its 
probability density function. Summing over all possible values of $X$ we obtain the marginal density of random vector $\textbf{Y}$:

\begin{equation}
    f_{\textbf{Y}}(y_1,\ldots,y_p)= \sum_{i=1}^{M} \mathbb{P}(X=i) f_{\textbf{Y}}(y_1,\ldots,y_p|X=i)
\end{equation}
    
where $\mathbb{P}(X=i) = \pi_i$ and $f_{\textbf{Y}}(y_1,\ldots,y_p|X=i) = f_i(y_1,\ldots,y_p)$ are the mixing proportions and component densities respectively thus arriving at the same definition 
as in Equation \ref{eq:mixture}.

\subsection{Gaussian Mixture Models}

The Gaussian mixture model (GMM) is a probabilistic model that assumes all the data points are generated 
from a mixture of a finite number of Gaussian distributions. The component distributions are often chosen to be members of the same parametric family,
such as Gaussian distributions with respective mean and covariance parameter. Assuming that the number of components is $K$
the probability density function of the Gaussian mixture model can be expressed as follows \citep{Bishop2006}:

\begin{equation}
    f(y|\mu,\Sigma) = \sum_{k=1}^{K} \pi_k \mathcal{N}(y|\mu_k, \Sigma_k)
\end{equation}

where $\mu_k$ and $\Sigma_k$ are the mean and covariance matrix (symmetric and positive semi-definite) of the $k$-th Gaussian component. 

Thus, the vector of parameters is $\theta = \{k \in \mathbb{N}: (\pi_k,\mu_k,\Sigma_k)\}$. Probability density function of each component of GMM depends on the dimensionality of the data.
Let the dimensionality of the data be $d>1$ then the random vector $\textbf{Y}=(y_1,\ldots,y_d)$ has the probability density function of the $k$-th component 
defined as following multivariate Gaussian distribution:

\begin{equation}
    f_k(\textbf{Y}) = \frac{1}{{(2\pi)}^{d/2}|\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}{(\textbf{Y}-\mu_k)}^T\Sigma_k^{-1}(\textbf{Y}-\mu_k)\right)
\end{equation}

where $|\Sigma_k|$ denotes the determinant of the covariance matrix $\Sigma_k$. Therefore, the conditional distribution of $Y$ given component $k$ is:

\begin{equation}
    \textbf{Y}|k \sim \mathcal{N}_d(\mu_k,\Sigma_k)
\end{equation}

\section{Poisson HMM}

\section{Context-sensitive HMM}
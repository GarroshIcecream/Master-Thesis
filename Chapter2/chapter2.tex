%!TEX root = ../thesis.tex


\if pdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

\chapter{Hidden Markov Models}

Until now, we have considered visible states in a sense that the sequence of states was known, we refer to these models as \textit{visible Markov Models}.
In this section we will consider a situation in which we do not observe the states directly but only as a "guess" given other visible observations that are available to us. 
These "visible" observations are labeled as emissions emitted by the respective hidden state. Thus, the observations are assumed to be generated by certain hidden stochastic process, i.e. a Markov Chain.\footnote{Models that
comprise unobserved random variables, e.g. Hidden Markov Models, are called \textbf{latent variable models}, \textbf{missing data models}, or also \textbf{models with incomplete data}}
In general, we could assume that the state and emission space is not necessarily finite, but we will follow the classical definition by \cite{Rabiner1989} and \cite{Elliott1995} of Hidden Markov Models and assume that at least state space is finite.

\section{Discrete Hidden Markov Model}

Revisiting previous section, we have defined a transition matrix $\textbf{A}$ and initial distribution $\textbf{p}$ for a homogeneous Markov Chain. Both of these parameters are used to describe the hidden stochastic process.
In order to describe the emissions, we will define an $N \times M$ \textit{emission matrix} $\textbf{B}$\footnote{Also referred to as \textbf{observation matrix} and \textbf{observations} respectively} as follows:

\begin{equation}
    \textbf{B} = \begin{pmatrix}
    b_1(y_1) & b_1(y_2) & \ldots & b_1(y_M) \\
    b_2(y_1) & b_2(y_2) & \ldots & b_2(y_M) \\
    \vdots & \vdots & \ddots & \vdots \\
    b_N(y_1) & b_N(y_2) & \ldots & b_N(y_M) \\
    \end{pmatrix}
\end{equation}

\noindent where $N$ represents number of states and $M$ number of possible emission symbols.

Probabilistically each element of the matrix represents conditional probability of emitting symbol $k$ for all $k = 1,2,\ldots,M$, given state $i$: 

\begin{equation}
    b_{i}(k) = \mathbb{P}(Y_t = k|X_t = i) 
\end{equation}

As stated at the beginning of this section, observer does not have access to the hidden states, but only to the emissions emitted by the hidden states. 
Thus, observing only another stochastic process $\{Y_t, t \in \mathbb{N}_0\}$ linked to hidden Markov Chain s.t. it governs the distribution of $Y_t$. 
In other words, entire statistical inference, even in terms of the hidden Markov process, is based on the observed sequence of emissions $\{Y_t\}$.

Considering the discrete time index $t$, Hidden Markov model is bivariate discrete-time stochastic process $\{X_t,Y_t\}$, where $\{Y_t\}$ is a sequence of conditionally 
independent and identically distributed random variables with a probability distribution determined by the hidden state $i$ at time $t$. \cite{Rabiner1989}

Note that there are three common types of Hidden Markov Models depending on structure of the underlying hidden Markov Chain according to \cite{Nelwamondo2006}. These are namely (a) left-to-right, 
(b) two-parallel left-to-right and (c) ergodic. First model bears a property that the next state index is always greater than or equal to the current state index s.t. the final state is absorbing.
Two-parallel left-to-right model allows for two parallel paths taken by the Markov Chain and lastly in ergodic model, all states are connected.
For the purpose of this work we will only consider models with ergodic property as shown in Figure \ref{fig:ergodic}.

Above we specified an emission matrix $\textbf{B}$ as a discrete probability distribution of the emissions, given the hidden state, that take on values from a finite set of $M$ possible emissions. $\{Y_t\}$ is then a sequence of conditionally 
independent and identically distributed discrete random variables that follow a categorical distribution with $M$ possible outcomes, i.e. each emission symbol $k$ is given a probability of being emitted by the hidden state $i$
 as stated by \cite{Paisley2009}:

\begin{gather}
    Y | X = i \sim \text{Cat}(M;b_i(y_1),\ldots,b_i(y_M)), \quad \forall i \in I \\
    f(y|x=i) = \prod\limits_{k=1}^M b_i(k)^{\mathbbm{1}{\{y=k\}}}
\end{gather}
 
Important property of the categorical distribution in Bayesian statistics is that it is a generalization of the Bernoulli distribution for $M > 2$ 
outcomes and also a conjugate prior for the Dirichlet distribution. In some instances, as expressed by \cite{Paisley2009}, we also use Dirichlet distribution with parameters $\alpha_1=\ldots=\alpha_M$ 
as it is a special case called uniform prior for the categorical distribution. The same approach is also applicable other parameters of the model, i.e. 
transition matrix $\textbf{A}$ and initial distribution $\textbf{p}$.

Usually, as mentioned in \cite{Agresti2007}, we generalize the categorical distribution to a multinomial distribution in which case the number of samples is fixed to 1. 
In other words, the emission space is not 1-to-M encoded but rather 1-of-M encoded s.t. the probability mass function is:

\begin{gather}
    Y | X = i \sim \text{Multinomial}(1;M;b_i(y_1),\ldots,b_i(y_M)), \quad \forall i \in I \\
    f(\textbf{y},n=1|x=i) = \prod_{k=1}^M b_i(k)^{y_{k}}, \quad \sum_{k=1}^M y_{k} = 1
\end{gather}

\noindent where $\textbf{y}$ is a vector of length $M$ with only one element equal to 1 and the rest 0. 

HMM with finite state and emission space as described above is also called \textit{discrete Hidden Markov Model}. Same definition is also applicable for continuous emission space, where the probability density function is 
often a Gaussian distribution with mean vector $\mathbf{\mu}$ and covariance matrix $\Sigma$ or Poisson distribution with parameter vector $\boldsymbol{\lambda}$.

An important note, as per \cite{Bishop2006} and \cite{}, is that the emissions are not necessarily independent, but only conditionally independent given the hidden state, which implies that the process $\{Y_t\}$ is not a Markov Chain, as opposed to process $\{X_t\}$,
but we may view it as an extension of Mixture Models with certain dependencies between the emissions. For example, in a case where we have a Markov Chain with finite $N$ hidden states and emissions modelled by Gaussian distributions, 
we refer to such model as \textit{Gaussian Hidden Markov Model}\footnote{In some literature as \cite{Capp2005} also \textbf{normal Hidden Markov Model}} s.t. marginal distribution of $Y_t$ is a mixture of Gaussian distributions.

Since HMM is categorized as a sequence model, we are generally interested in a joint probability distribution of the hidden states and the emissions when examining 
time series data or any other sequential data. Such joint probability distribution given parameters vector $\theta = (\textbf{A},\textbf{B},\textbf{p})$ is, as described by \cite{Rabiner1989}, 
expressed by the following equation:

\begin{align}
    \mathbb{P}(\textbf{X}, \textbf{Y}| \theta) = & \mathbb{P}(X_0 = x_0) \mathbb{P}(Y_0 = y_0|X_0 = x_0) \notag \\ 
                                                 & \prod_{t=1}^T \mathbb{P}(X_t = x_t|X_{t-1} = x_{t-1}) \mathbb{P}(Y_t = y_t|X_t = x_t) \\
                                               = & p_{x_0} b_{x_0}(y_0) \prod_{t=1}^T a_{x_t,x_{t-1}} b_{x_t}(y_t)
\end{align}

\noindent where $\textbf{X} = \{x_0,x_1,\ldots,x_T\}$ and $\textbf{Y} = \{y_0,y_1,\ldots,y_T\}$ are the sequences of hidden states and emissions respectively. 

One possibility of graphically representing such joint probability distribution is by using a graphical 
model as shown in Figure \ref{fig:HMM}. 

\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,semithick]
    \node[state, minimum size=1.2cm]         (A)                    {$x_{t-1}$};
    \node[state, minimum size=1.2cm]         (B) [right of=A]       {$x_{t}$};
    \node[state, minimum size=1.2cm]         (C) [right of=B]       {$x_{t+1}$};

    \node[left of=A]  (prev) {$\dots$};
    \node[right of=C] (next) {$\dots$};

    \path (prev) edge node [above] {} (A)
    (A)    edge node [above] {} (B)
    (B)    edge node [above] {} (C)
    (C)    edge node [above] {} (next);

    \node [draw,circle,below of=A, node distance=2cm, minimum size=1.2cm] (WA) {$y_{t-1}$};
    \node [draw,circle,below of=B, node distance=2cm, minimum size=1.2cm] (WB) {$y_{t}$};
    \node [draw,circle,below of=C, node distance=2cm, minimum size=1.2cm] (WC) {$y_{t+1}$};

    \path (A) edge [->] (WA)
    (B) edge [->] (WB)
    (C) edge [->] (WC);
    \end{tikzpicture}
\end{center}
\caption{An HMM with 4 hidden states and 2 discrete emissions denoted by $x_1$ and $x_2$.}
\label{fig:HMM}
\end{figure}

We may also visualize conditional relationship with a graphical model in Figure \ref{fig:ergodic} representing a structure of HMM with 3 hidden states $\{S_1,S_2,S_3\}$ and 2 emission symbols $\{E_1,E_2\}$ where the nodes represent the relationship 
imposed by the transition matrix $\textbf{A}$ and emission matrix $\textbf{B}$:

\begin{figure}[htbp]
    \begin{center}
    \begin{tikzpicture}[]
    % states
    \node[state, minimum size=1.2cm] (s1) at (0,2) {$S_1$}
        edge [loop above]  node[auto,above] {$a_{11}$}();
    \node[state, minimum size=1.2cm] (s2) at (3,2) {$S_2$}
        edge [<-,bend right=45] node[auto,swap] {$a_{12}$} (s1)
        edge [->,bend left=45] node[auto,above] {$a_{21}$} (s1)
        edge [loop above] node[auto,above] {$a_{22}$} ();
    \node[state, minimum size=1.2cm] (s3) at (6,2) {$S_3$}
        edge [<-,bend right=70] node[auto,swap] {$a_{13}$} (s1)
        edge [->,bend left=60] node[auto,above] {$a_{31}$}(s1)
        edge [<-,bend right=45] node[auto,swap] {$a_{23}$} (s2)
        edge [->,bend left=45] node[auto,above] {$a_{32}$} (s2)
        edge [loop above] node[auto,above] {$a_{33}$} ();
    % observations
    \node[observation, minimum size=1cm] (y1) at (1.5,-1.5) {$E_1$}
        edge [lightedge] node[auto,below,xshift=-20pt] {$b_{S_1}(E_1)$} (s1)
        edge [lightedge] (s2)
        edge [lightedge] (s3);
    \node[observation, minimum size=1cm] (y2) at (4.5,-1.5) {$E_2$}
        edge [lightedge] (s1)
        edge [lightedge] (s2)
        edge [lightedge] node[auto,swap] {$b_{S_3}(E_2)$} (s3);
    \end{tikzpicture}
    \end{center}
    \caption{An HMM with 3 hidden states and 2 emission symbols denoted by $E_1$ and $E_2$.}
    \label{fig:ergodic}
    \end{figure}
    

There are 3 main assumptions of Hidden Markov Models as a consequence of the properties of Markov processes,
as suggested by \cite{Oliver2013} and \cite{}:

\begin{itemize}
    \item[1)] \textbf{Markov memoryless assumption} - this assumption states that the next hidden state $X_{t+1}$ depends only on the current state $X_t$, so that the transition probabilities are defined as:

        \begin{equation}
            \mathbb{P}(X_{t+1} = j| X_{t} = i,X_{t-1} = i_{t-1}, \ldots,X_{0} = i_0) = \mathbb{P}(X_{t+1} = j| X_{t} = i) \quad \forall i,j \in I
        \end{equation}

        It is also possible to assume that the states in HMM are dependent beyond the current state therefore giving rise to \textit{k-order HMM} 
        where the conditional distribution of the next state depends on the current state and the previous $k$ states $X_{t-k}, X_{t-k+1}, \ldots, X_{t-1}$. \cite{Capp2005}

    \item[2)] \textbf{Stationary assumption} - The transition matrix $\textbf{A}$ is time invariant s.t. transition probabilities only depend on the time interval between the transitions, 
    thus for all $t \neq s$:

        \begin{equation}
            \mathbb{P}(X_{t+1} = j| X_{t}= i) = \mathbb{P}(X_{s+1} = j| X_s = i) \quad \forall i,j \in I
        \end{equation}

        However, when we abstract from this assumption, we may consider a \textit{non-stationary HMM} where the transition matrix is time-dependent, i.e. depends on time index $t$.

    \item[3)] \textbf{Emission independence assumption} - Emissions are independent, s.t. if we have an emission sequence $\textbf{Y} = \{y_1,y_2,\ldots,y_T\}$ then:

        \begin{equation}
            \mathbb{P}(\textbf{Y}|X_1 = x_1,\ldots,X_T = x_T) = \prod_{t=1}^T \mathbb{P}(Y_t = y_t|X_t = x_t)
        \end{equation}

        When there is a dependence between the emissions in a way that the emission at time $t$ depends on the previous emissions, conditionally on the hidden state sequence $\{X_k\}$, $\{Y_k\}$
        forms a (non-homogeneous) Markov Chain, therefore jointly representing a generalization of HMM called \textit{Markov-switching Models}\footnote{Also, \textbf{Markov jump system}.}. \cite{Capp2005}

\end{itemize}

There are mainly 3 fundamental problems in HMM that need to be resolved as depicted by \cite{Oliver2013} and \cite{Ching2005}:
\begin{itemize}
\item[1.] \textbf{Evaluation problem} - Given a model denoted as $\theta = (\textbf{A},\textbf{B},\textbf{p})$ and an emission sequence $\textbf{Y} = y_1, y_2,\ldots,y_T$, how to efficiently compute the probability that the model generated the observation sequence, in other words, what is $\mathbb{P}(\textbf{Y}|\theta)$? 
\item[2.] \textbf{Decoding problem} - What is the most likely sequence of hidden states that could have generated the emission sequence \textbf{Y}? Thus, we would like to find $\textbf{X} = \underset{\textbf{X}}{\arg\max} \mathbb{P}(\textbf{Y},\textbf{X}|\theta)$, where $\textbf{X}$ is the hidden state sequence. 
\item[3.] \textbf{Learning problem} - Given a set of emission sequences find $\theta$ that best explains the emission sequence $\textbf{Y}$. Thus, find the vector of parameters $\theta$ that maximizes $\mathbb{P}(\textbf{Y}|\theta)$. 
\end{itemize}

The most traditional approaches in solving these 3 fundamental problems differ and one may not suffice in solving all three. 
The evaluation problem is usually solved by \textbf{Forward-Backward algorithm}\footnote{Focusing on the most widely used implementation called \textbf{alpha-beta algorithm}.}, the decoding problem by well-known \textbf{Viterbi algorithm} 
and the last learning problem by \textbf{Baum-Welsch algorithm} which is a special case of Expectation-maximization (EM) algorithm.
 All three algorithms are described in the following chapter.

\section{Gaussian HMM}

As shown in the last chapter the joint probability distribution of the complete 
data is not strictly concave and unimodal thus we can not guarantee the convergence to the global optimum.  
Fig 2.2 appropriately displays possible shape of the joint probability distribution of the complete data with multiple 
local optima of the log-likelihood function caused by the hidden states. 
Such probability distributions are often called mixture distributions. 

They were implicitly introduced in the last chapter while estimating the parameters of the Hidden Markov Model. 
Although, most of the time we consider unimodal distributions for our data it might not be the case for particular data sets.
Therefore, certain phenomena can be better described by mixture distributions, but their use should be justified theoretically or empirically. 
Statistical inferences about the subpopulations within the overall population require such tools in cases where the subpopulations significantly differ 
and may be interpreted the same way as hidden variables in case of Hidden Markov Models.
In this section, we first introduce the concept of \textit{mixture distributions}, then we will focus on \textit{Gaussian mixture models }
and their direct relation to \textit{Gaussian Hidden Markov Models}.

\subsection{Mixture Distributions}

Let $Y_j$ denote a p-dimensional random vector with probability density function $f(y_j)$ on $\mathbb{R}^p$  
selected from random sample $Y_1,\ldots,Y_n$. A realization of such a random vector is denoted by $y_j$.

Mixture distribution or density is defined as a convex combination of $g$ component distributions or densities respectively 
as follows:

\begin{equation}
f(y_j) = \sum_{i=1}^{g} \pi_i f_i(y_j)
\end{equation}

where $f_i(y_j)$ are component densities of the mixture and $\pi_i$ are mixing proportions or mixture weights with following properties:

\begin{equation}
    0 \leq \pi_i \leq 1 \quad \forall i \in \{1,2,\ldots,g\}
\end{equation}

and

\begin{equation}
    \sum_{i=1}^{g} \pi_i = 1
\end{equation}

Therefore, the density (3.1) of $f(y_j)$ is referred to as a g-component finite mixture density, conversely $F(y_j)$ 
as a g-component finite mixture distribution. 

Formula (3.1) assumes that the number of components $g$ is fixed. In many applications this is not the case, and we have to infer 
the number of components from the data. Furthermore, mixing proportions $\pi_i$ are also unknown and have to be estimated along with 
the respective parameters of the component densities $f_i(y_j)$.

Analogously to previous definition we can define a finite mixture of random variables $Y_j$ as follows:

\begin{equation}
    f(y_j,z) =  g(z) f(y_j|z)
\end{equation}

where $Z$ is defined as a random variable with multinomial distribution with a vector of parameters $\pi = \{\pi_1,\ldots,\pi_g\}$ and $g(z)$ as its probability density function.
Summing over all possible values of $Z$ we obtain the marginal density of random vector $Y_j$:

\begin{equation}
    f(y_j) = \sum_{i=1}^{g} P(Z=i) f(y_j|Z=i)
\end{equation}
    
where $P(Z=i) = \pi_i$ and $f(y_j|Z=i) = f_i(y_j)$ are the mixing proportions and component densities respectively Therefore
seeing the equivalence of Equation (3.1) and (3.4).

\subsection{Gaussian Mixture Models}
The Gaussian mixture model (GMM) is a probabilistic model that assumes all the data points are generated 
from a mixture of a finite number of Gaussian distributions. The component distributions are often chosen to be members of the same parametric family,
such as Gaussian distributions with respective mean and covariance parameter. Assuming that the number of components is $K$
the probability density function of the Gaussian mixture model can be expressed as follows:

\begin{equation}
    f(x|\mu,\Sigma) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)
\end{equation}

where $\mu_k$ and $\Sigma_k$ are the mean and covariance matrix (symmetric and positive semi-definite) of the $k$-th Gaussian component. 

Thus, the vector of parameters is $\theta = \{k \in \mathbb{N}: (\pi_k,\mu_k,\Sigma_k)\}$. Probability density function of each component of GMM depends on the dimensionality of the data.
Let the dimensionality of the data be $d>1$ then the random vector $Y=(y_1,\ldots,y_d)$ has the probability density function of the $k$-th component 
defined as following multivariate Gaussian distribution:

\begin{equation}
    f_k(y) = \frac{1}{{(2\pi)}^{d/2}|\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}{(y_j-\mu_k)}^T\Sigma_k^{-1}(y_j-\mu_k)\right)
\end{equation}

where $|\Sigma_k|$ denotes the determinant of the covariance matrix $\Sigma_k$.

\subsection{Motivating EM algorithm}
\cite{Bishop2006}
Given the data $\textbf{Y} = \{\textbf{Y}_1,\ldots,\textbf{Y}_N\}$, where each element is an independently and identically distributed 
p-dimensional random vector, s.t. $Y_n \in \mathbb{R}^p$ and:

\begin{equation}
    \textbf{Y}_n \sim \mathcal{N}_p(\mu, \Sigma) \quad \forall n \in \{1,\ldots,N\} 
\end{equation}

The goal is to estimate $\theta$ of the mixture model, 
i.e.\ for each mixture component $k$ we need to estimate its mean $\mu_k$, covariance matrix $\Sigma_k$ and mixing proportion $\pi_k$. 

Note that now we will assume that each element of random vector $\textbf{Y}_n$ is generated by one of the mixture components, and we know which one. 
This is called the complete data since we have information about the hidden variable $Z$ which indicates the component from which the data point was generated.
Therefore, the complete data likelihood function is defined as follows: 

\begin{equation} \label{eq:likelihood-gaussian}
    L_c(\theta|\textbf{Y},\textbf{Z}) = \prod_{i=1}^{N} \sum_{k=1}^{K} \pi_k \mathcal{N}(\textbf{Y}_i|\mu_k,\Sigma_k)
\end{equation}

and the log-likelihood function is:

\begin{equation}
    \ell_c(\theta|\textbf{Y},\textbf{Z}) = \sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_k \mathcal{N}(\textbf{Y}_n|\mu_k,\Sigma_k)
\end{equation}

Equation (3.10), shows that the log-likelihood function has a form of a sum of logarithms which results in no strict analytical solution for the maximum likelihood estimation of model parameters.

Given the assumption that we know the hidden variable $\textbf{Z}$ directly from the data defined as a K-dimensional random vector $\textbf{Z}=(z_1,\ldots,z_K)$ 
with multinomial distribution and parameters $\boldsymbol{\pi} = \{\pi_1,\ldots,\pi_K\}$, the log-likelihood function can be rewritten as follows:

\begin{equation}
    \ell_c(\theta|\textbf{Y},\textbf{Z}) = \sum_{i=1}^{N} \sum_{k=1}^{K} z_{ik} \left(\log \pi_k + \log \mathcal{N}(\textbf{Y}_n|\mu_k,\Sigma_k)\right)
\end{equation}

with variable $z_{ik}$ defined as follows:

\begin{equation}
    z_{ik} = \begin{cases}
        1 & \text{if } \textbf{Y}_i \text{ was generated by the } k\text{-th component} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

also notice that for each sample, random vector $z_{i}$ is distributed identically and independently according to the multinomial distribution with vector of parameters $\boldsymbol{\pi}$.

\subsection{EM algorithm for GMM}

The EM algorithm for GMM is very similar to the one defined for Hidden Markov Models in the previous chapter. 
We start with the definition of the $\textbf{E-step}$ of the EM algorithm. The conditional expected complete 
data log-likelihood function given log-likelihood function by Equation (3.11) as follows:

\begin{equation}
    Q(\theta|\theta^{(t)}) = \mathbb{E}[\ell_c(\theta|\textbf{Y},\textbf{Z})|\textbf{Y},\theta^{(t)}]
\end{equation}

hence inserting Equation (3.11) into (3.13) we obtain:

\begin{equation}
    Q(\theta|\theta^{(t)}) = \sum_{i=1}^{N} \sum_{k=1}^{K} \mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}] \left(\log \pi_k + \log \mathcal{N}(\textbf{Y}_i|\mu_k,\Sigma_k)\right)
\end{equation}

where $\theta^{(t)}$ denotes the parameter vector at the $t$-th iteration of the algorithm and $\mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}]$ is the expected value of the hidden variable $z_{ik}$ given the data $\textbf{Y}_i$ and the parameter vector $\theta^{(t)}$.
The expected value of the hidden variable $z_{ik}$ is defined as follows:

\begin{equation} \label{eq:posterior_prob}
    \mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}] = P(z_{ik} = 1|Y_i,\theta^{(t)}) = \frac{\pi_k^{(t)} \mathcal{N}(\textbf{Y}_i|\mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^{K} \pi_j^{(t)} \mathcal{N}(\textbf{Y}_i|\mu_j^{(t)},\Sigma_j^{(t)})}
\end{equation}

where $\pi_k^{(t)}$, $\mu_k^{(t)}$ and $\Sigma_k^{(t)}$ are the mixing proportion, mean and covariance matrix of the $k$-th component at the $t$-th iteration of the algorithm respectively.
Taking a step back and looking at the Equation (3.5) we observe that the conditional expected value of $z_{ik}$ given data $\textbf{Y}_i$ 
and parameter vector $\theta^{(t)}$ is the posterior probability of the hidden variable $z_{ik}$.

The $\textbf{M-step}$ of the EM algorithm then aims to maximize the conditional expected complete data log-likelihood function $Q(\theta|\theta^{(t)})$ with respect to the parameter vector $\theta$.
Such maximization is formally defined as follows:

\begin{equation}
    \theta^{(t+1)} = \underset{\theta \in \Theta}{\arg\max} Q(\theta|\theta^{(t)})
\end{equation}

Taking partial derivatives of $Q(\theta|\theta^{(t)})$ with respect to the parameters $\pi_k$, $\mu_k$ and $\Sigma_k$ and setting them to zero we obtain the following equations:

\begin{equation}
    \hat{\pi}_k^{(t+1)} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}]
\end{equation}

\begin{equation}
    \hat{\mu}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}] \textbf{Y}_i}{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}]}
\end{equation}

\begin{equation}
    \hat{\Sigma}_k^{(t+1)} = \frac{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}](\textbf{Y}_i - \mu_k^{(t+1)}){(\textbf{Y}_i - \mu_k^{(t+1)})}^T}{\sum_{i=1}^{N} \mathbb{E}[z_{ik}|\textbf{Y}_i,\theta^{(t)}]}
\end{equation}


\section{Poisson HMM}

\section{Context-sensitive HMM}